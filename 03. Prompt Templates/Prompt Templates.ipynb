{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Announcement-on-ML\n",
    "<a href='http://www.kgptalkie.com' target=\"_blank\"> <img src='https://github.com/laxmimerit/Important-Announcement-on-ML/raw/master/kgptalkie_strips.png'/></a>\n",
    "\n",
    "# ML Resources\n",
    "|  ML Course | Description |\n",
    "|:---|:---|\n",
    "| [**Deploy LLM App with Ollama and Langchain in Production**](https://www.udemy.com/course/ollama-and-langchain/?referralCode=7F4C0C7B8CF223BA9327) | Master Langchain v0.3, Private Chatbot, Deploy LLM App.  Ollama, LLAMA, LLAMA 3.2, FAISS, RAG, Deploy RAG, Gen AI, LLM|\n",
    "| [**Fine Tuning LLM with HuggingFace Transformers for NLP**](https://www.udemy.com/course/fine-tuning-llm-with-hugging-face-transformers/?referralCode=6DEB3BE17C2644422D8E) | Learn how to fine tune LLM with custom dataset. You will learn basics of transformers then fine tune LLM|\n",
    "| [**Data Visualization in Python Masterclassâ„¢: Beginners to Pro**](https://bit.ly/udemy95off_kgptalkie) |  Learn to build Machine Learning and Deep Learning models using Python and its libraries like Scikit-Learn, Keras, and TensorFlow. |\n",
    "| [**Python for Machine Learning: A Step-by-Step Guide**](https://bit.ly/ml-ds-project) | Learn to build Machine Learning and Deep Learning models using Python and its libraries like Scikit-Learn, Keras, and TensorFlow. |\n",
    "| [**Deep Learning for Beginners with Python**](https://bit.ly/dl-with-python) | Neural Networks, TensorFlow, ANN, CNN, RNN, LSTM, Transfer Learning and Much More. |\n",
    "| [**Python for Linear Regression in Machine Learning**](https://bit.ly/regression-python) | Learn to build Linear Regression models using Python and its libraries like Scikit-Learn. |\n",
    "| [**Introduction to Spacy 3 for Natural Language Processing**](https://bit.ly/spacy-intro) | Learn to build Natural Language Processing models using Python and its libraries like Spacy. |\n",
    "| [**Advanced Machine Learning and Deep Learning Projects**](https://bit.ly/kgptalkie_ml_projects) | Learn to build Advanced Machine Learning and Deep Learning models using Python and transformer models like BERT, GPT-2, and XLNet. |\n",
    "| [**Natural Language Processing in Python for Beginners**](https://bit.ly/intro_nlp) | Learn to build Natural Language Processing Projects using Spacy, NLTK, and Gensim, and transformer models like BERT, GPT-2, and XLNet. |\n",
    "| [**Deployment of Machine Learning Models in Production in Python**](https://bit.ly/bert_nlp) |  Learn to deploy Machine Learning and Deep Learning models using Python and its libraries like Flask, Streamlit, and NGINX. |\n",
    "| [**R 4.0 Programming for Data Science - Beginners to Pro**](https://bit.ly/r4-ml) | Learn to build Machine Learning and Deep Learning models using R and its libraries like caret, tidyverse, and keras. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Role       | Description                                                                                                                                                   |\n",
    "|------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| system     | Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.                                          |\n",
    "| user       | Represents input from a user interacting with the model, usually in the form of text or other interactive input.                                              |\n",
    "| assistant  | Represents a response from the model, which can include text or a request to invoke tools.                                                                    |\n",
    "| tool       | A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support tool calling. |\n",
    "| function (legacy) | This is a legacy role, corresponding to OpenAI's legacy function-calling API. tool role should be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Message Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Message Type        |    Corresponding Role       |    Description                                                                                                          |\n",
    "|------------------------|-----------------------------|--------------------------------------------------------------------------------------------------------------------------|\n",
    "|    SystemMessage       |    system                   |    Corresponds to the system role.                                                                                       |\n",
    "|    HumanMessage        |    user                     |    Corresponds to the user role.                                                                                         |\n",
    "|    AIMessage           |    assistant                |    Corresponds to the assistant role.                                                                                    |\n",
    "|    AIMessageChunk      |    assistant                |    Corresponds to the assistant role, used for streaming responses.                                                      |\n",
    "|    ToolMessage         |    tool                     |    Corresponds to the tool role.                                                                                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dotenv import load_dotenv\n\nload_dotenv('./../.env')\n\nimport os\n# Do not print sensitive API keys\napi_key = os.environ.get(\"LANGSMITH_API_KEY\")\nprint(\"API key loaded successfully\" if api_key else \"No API key found\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'qwen3'\n",
    "\n",
    "llm = ChatOllama(base_url=base_url, model=model)\n",
    "\n",
    "question = 'tell me about the earth in 3 points'\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'sherlock'\n",
    "\n",
    "llm = ChatOllama(base_url=base_url, model=model)\n",
    "\n",
    "question = 'tell me about the earth in 3 points'\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'sheldon'\n",
    "\n",
    "llm = ChatOllama(base_url=base_url, model=model)\n",
    "\n",
    "question = 'tell me about the earth in 3 points'\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'qwen3'\n",
    "\n",
    "llm = ChatOllama(base_url=base_url, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = HumanMessage('tell me about the earth in 3 points')\n",
    "system = SystemMessage('You are elemetary teacher. You answer in short sentences.')\n",
    "\n",
    "messages = [system, question]\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = HumanMessage('tell me about the earth in 3 points')\n",
    "system = SystemMessage('You are phd teacher. You answer in short sentences.')\n",
    "\n",
    "messages = [system, question]\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Prompt Templates\n",
    "\n",
    "|    Prompt Template Class             |    Description                                                                                                     |\n",
    "|--------------------------------------|---------------------------------------------------------------------------------------------------------------------|\n",
    "|    SystemMessagePromptTemplate       |    Template for generating system messages that provide model context or instructions.                            |\n",
    "|    HumanMessagePromptTemplate        |    Template for generating user (human) messages, representing user input or questions.                          |\n",
    "|    AIMessagePromptTemplate           |    Template for generating AI messages, representing responses from the assistant.                                |\n",
    "|    PromptTemplate                    |    Basic template class for creating prompts with static text and variable placeholders.                          |\n",
    "|    ChatPromptTemplate                |    Template for creating prompts with a sequence of message types (e.g., system, user, assistant) in a chat format. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "                                        SystemMessagePromptTemplate,\n",
    "                                        HumanMessagePromptTemplate,\n",
    "                                        PromptTemplate,\n",
    "                                        ChatPromptTemplate\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question.format(topics='sun', points=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.format(school='elemetary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [system, question]\n",
    "\n",
    "template = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = template.invoke({'school': 'elementary', 'topics': 'sun', 'points': 5})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-and-ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}