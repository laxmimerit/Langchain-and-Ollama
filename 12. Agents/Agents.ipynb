{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# LangChain Agents: Complete Guide\n",
    "\n",
    "This notebook demonstrates three different approaches to creating LangChain agents:\n",
    "1. **Basic Agent with Model String** - Simplest approach for quick prototyping\n",
    "2. **Agent with Explicit Model Instance** - Full control over model configuration\n",
    "3. **Dynamic Model Selection** - Cost optimization with automatic model switching\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have the required packages installed and Ollama running:\n",
    "\n",
    "```bash\n",
    "pip install -U langchain langchain-community langchain-core langgraph\n",
    "pip install -U ddgs python-dotenv\n",
    "```\n",
    "\n",
    "Local LLM Serving with Ollama:\n",
    "```bash\n",
    "ollama pull qwen3\n",
    "ollama pull gpt-oss\n",
    "ollama serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Basic Agent with Model String\n",
    "\n",
    "The simplest way to create a LangChain agent using a model string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from langchain.agents import create_agent\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-tool1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the web search tool\n",
    "print(tools.web_search.invoke(\"python programming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with model string - simplest approach\n",
    "agent = create_agent(\n",
    "    \"ollama:qwen3\",\n",
    "    tools=[tools.web_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invoke1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a search query\n",
    "result = agent.invoke({\n",
    "    \"messages\": \"Search for python tutorials\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Agent with Explicit Model Instance\n",
    "\n",
    "Full control over model configuration by creating an explicit ChatOllama instance.\n",
    "\n",
    "## Model Parameters Explained\n",
    "- **temperature**: Controls randomness (0.0 = deterministic, 1.0 = very creative)\n",
    "- **num_predict**: Maximum tokens to generate (similar to max_tokens in OpenAI)\n",
    "- **top_k**: Number of highest probability tokens to consider\n",
    "- **top_p**: Cumulative probability threshold for token selection\n",
    "- **repeat_penalty**: Penalty for repeating tokens\n",
    "- **num_ctx**: Context window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-tool2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the calculate tool\n",
    "tools.calculate.invoke(\"3*4+2-10/20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance with custom parameters\n",
    "model = ChatOllama(\n",
    "    model=\"qwen3\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with explicit model instance\n",
    "agent = create_agent(model, tools=[tools.calculate])\n",
    "\n",
    "print(\"Agent created with explicit model instance!\")\n",
    "print(\"Full control over temperature, token limits, and other parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invoke2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a mathematical calculation\n",
    "result = agent.invoke({\n",
    "    \"messages\": \"What's 15 * 27 + 100?\"\n",
    "})\n",
    "\n",
    "print(f\"Response: {result['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "result2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View full result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-print2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print the last message\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiments",
   "metadata": {},
   "source": [
    "## Experimenting with Different Settings\n",
    "\n",
    "Let's compare different model configurations to see how they affect the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Explain what 2 + 2 equals and show your reasoning\"\n",
    "\n",
    "# Configuration 1: Conservative/Deterministic\n",
    "print(\"=== Configuration 1: Conservative/Deterministic ===\")\n",
    "llm1 = ChatOllama(\n",
    "    model=\"qwen3\",\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    repeat_penalty=1.1,\n",
    "    num_predict=500,\n",
    "    num_ctx=4096,\n",
    "    reasoning=True\n",
    ")\n",
    "agent = create_agent(llm1, tools=[tools.calculate])\n",
    "result = agent.invoke({\"messages\": question})\n",
    "print(f\"Conservative output: {result['messages'][-1].content}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Configuration 2: Balanced/Production ===\")\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3\",\n",
    "    temperature=2,\n",
    "    top_k=2000,\n",
    "    repeat_penalty=1.15,\n",
    "    repeat_last_n=64,\n",
    "    num_predict=1000,\n",
    "    num_ctx=8192,\n",
    "    keep_alive=\"5m\",\n",
    "    reasoning=True\n",
    ")\n",
    "\n",
    "agent = create_agent(llm, tools=[tools.calculate])\n",
    "result = agent.invoke({\"messages\": question})\n",
    "print(result['messages'][-1].content)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "# Part 3: Dynamic Model Selection (Qwen3 → GPT-OSS)\n",
    "\n",
    "Cost-optimization strategy where the agent automatically switches between models based on conversation complexity.\n",
    "\n",
    "## Selection Logic\n",
    "- **< 10 messages**: Use Qwen3 (fast, efficient)\n",
    "- **≥ 10 messages**: Use GPT-OSS (better reasoning, longer context)\n",
    "\n",
    "## Real-World Applications\n",
    "- Customer service bots (simple queries → Qwen3, complex issues → GPT-OSS)\n",
    "- Research assistants (quick facts → Qwen3, analysis → GPT-OSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.runtime import Runtime\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "select-model",
   "metadata": {},
   "source": [
    "## Model Selection Function\n",
    "\n",
    "This function automatically chooses between Qwen3 and GPT-OSS based on conversation length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selection-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tool list for both models\n",
    "tool_list = [tools.web_search, tools.calculate]\n",
    "\n",
    "def select_model(state: AgentState, runtime: Runtime) -> ChatOllama:\n",
    "    \"\"\"Choose between Qwen3 and GPT-OSS based on conversation length.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    message_count = len(messages)\n",
    "    \n",
    "    if message_count < 10:\n",
    "        print(f\"Using Qwen3 for {message_count} messages\")\n",
    "        return ChatOllama(model=\"qwen3\", temperature=0.1).bind_tools(tool_list)\n",
    "    else:\n",
    "        print(f\"Switching to GPT-OSS for {message_count} messages\")\n",
    "        return ChatOllama(model=\"gpt-oss\", temperature=0.0, num_predict=2000).bind_tools(tool_list)\n",
    "\n",
    "print(\"Model selection function defined!\")\n",
    "print(\"Logic: < 10 messages = Qwen3, >= 10 messages = GPT-OSS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-dynamic",
   "metadata": {},
   "source": [
    "## Creating the Dynamic Agent\n",
    "\n",
    "Create an agent that uses our dynamic model selection function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with dynamic model selection\n",
    "agent = create_agent(select_model, tools=tool_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_conversation_progression(messages):\n",
    "    \"\"\"Demonstrate how the agent switches models as conversation grows.\"\"\"\n",
    "        \n",
    "    # Create a mock state to test model selection\n",
    "    mock_state = {\"messages\": messages}\n",
    "    \n",
    "    agent = create_agent(select_model, tools=tool_list)\n",
    "    result = agent.invoke(mock_state)\n",
    "\n",
    "    return result\n",
    "\n",
    "messages = [\n",
    "        \"Hello\", \"How are you?\", \"What's the weather?\", \"My name is Laxmi Kant Tiwari\",\n",
    "        \"Explain machine learning\", \"What about deep learning?\", \"Show me examples\",\n",
    "        \"How does this work?\", \"Give me more details\"]\n",
    "\n",
    "print(f\"Len of messages: {len(messages)}\")\n",
    "\n",
    "result = demo_conversation_progression(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        \"Hello\", \"How are you?\", \"What's the weather?\", \"My name is Laxmi Kant Tiwari\",\n",
    "        \"Explain machine learning\", \"What about deep learning?\", \"Show me examples\",\n",
    "        \"How does this work?\", \"Give me more details\", \"What is my name?\"]\n",
    "\n",
    "print(f\"Len of messages: {len(messages)}\")\n",
    "\n",
    "result = demo_conversation_progression(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-and-ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
