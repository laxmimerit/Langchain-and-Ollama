{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– LLM and AI Agent Development Courses  \n",
    "**âœ¨ All courses available for just â‚¹399 INR / $9.99 USD âœ¨**\n",
    "\n",
    "| Course | Description | Enroll |\n",
    "|--------|-------------|---------|\n",
    "| **ðŸŽ¯ Master OpenAI Agent Builder** | Build and deploy AI agents visually using OpenAI Agent Builder, ChatKit, RAG, Chatbot, AI Assistant with MCP, AWS, RDS MySQL | [Enroll Now](https://kgptalkie.com/agent-builder) |\n",
    "| **ðŸ”¥ MCP Mastery** | Build MCP servers & clients with Python, Streamlit, ChromaDB, LangChain, LangGraph agents, and Ollama integrations | [Enroll Now](https://kgptalkie.com/mcp) |\n",
    "| **ðŸ“Š Private Agentic RAG with LangChain** | Step-by-Step Guide to RAG with LangChain v1, LangGraph, and Ollama (Qwen3, Gemma3, DeepSeek-R1, LLAMA, FAISS) | [Enroll Now](https://kgptalkie.com/agentic-rag) |\n",
    "| **ðŸ”§ Master LangGraph and LangChain** | Agentic RAG and Chatbot, AI Agent with LangChain v1, Qwen3, Gemma3, DeepSeek-R1, LLAMA 3.2, FAISS Vector Database | [Enroll Now](https://kgptalkie.com/langgraph) |\n",
    "| **âš¡ Master Langchain and Ollama** | Master Langchain v1, Local LLM Projects with Ollama, Qwen3, Gemma3, DeepSeek-R1, LLAMA 3.2, Complete Integration Guide | [Enroll Now](https://kgptalkie.com/langchain) |\n",
    "| **ðŸ”¬ Fine Tuning LLM** | Learn transformer architecture fundamentals and fine-tune LLMs with custom datasets | [Enroll Now](https://kgptalkie.com/fine-tuning-llm) |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒ Join the Community & Stay Connected\n",
    "\n",
    "- ðŸ”— **Join the Discord Community:** https://discord.gg/RFjwbkNa  \n",
    "- ðŸ“º **Subscribe on YouTube (63K+ learners):** http://www.youtube.com/@KGPTalkie\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft Office Files PPT Excel and Word Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:**\n",
    "Unstructured Data Reader Setup\n",
    "\n",
    "https://python.langchain.com/docs/integrations/providers/unstructured/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 1: Key Notes and Script Generation for PPT Presentor (Speaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "OSError: No such file or directory: 'C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\\PY3_tab'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error Handling:\n",
    "C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt --> PY3 to PY3_tab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\laxmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unstructured openpyxl python-magic python-pptx\n",
    "# pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "\n",
    "loader = UnstructuredPowerPointLoader(\"data/ml_course.pptx\", mode=\"elements\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning Model Deployment'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = docs[0]\n",
    "doc.metadata\n",
    "doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_data = {}\n",
    "for doc in docs:\n",
    "    page = doc.metadata[\"page_number\"]\n",
    "    ppt_data[page] = ppt_data.get(page, \"\")  + \"\\n\\n\" + doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '\\n\\nMachine Learning Model Deployment\\n\\nIntroduction to ML Pipeline\\n\\nhttps://bit.ly/bert_nlp\\n\\n',\n",
       " 2: '\\n\\nWhat is Machine Learning Pipeline?\\n\\n',\n",
       " 3: '\\n\\nType of ML Deployment\\n\\nBatch: In batch deployment, ML models process large volumes of data at scheduled intervals, ideal for tasks like end-of-day reporting or monthly analytics.\\n\\nStream: Stream deployment enables ML models to process and analyze data in real-time as it flows in, suitable for applications like fraud detection or live social media analysis.\\n\\nRealtime: Realtime deployment allows ML models to provide instant predictions or decisions in response to incoming data, essential for use cases like recommendation systems or autonomous driving.\\n\\nEdge: Edge deployment involves running ML models on local devices close to the data source, reducing latency and bandwidth usage, which is crucial for IoT applications and smart devices.\\n\\n',\n",
       " 4: '\\n\\nInfrastructure and Integration\\n\\nHardware and Software: Setting up the right environment for model deployment.\\n\\nIntegration: Seamlessly integrating the model with existing systems and applications.\\n\\n',\n",
       " 5: '\\n\\nBenefits of Deploying ML Models\\n\\nFocus on new models, not maintaining existing models || Prevention of bugs || Creation of records for debugging and reproducing results || Standardization || Allows models to handle real-time data and large user bases.\\n\\n',\n",
       " 6: '\\n\\nChallenges in ML Deployment\\n\\nData Management: Making sure the model gets the right kind of data.\\n\\nModel Scalability and Performance: Ensuring that their model can effectively scale as it keeps adding more complex information.\\n\\nIntegration with Existing Systems: Fitting the model into current computers and software.\\n\\nMonitoring and Maintenance: Watching and fixing the model over time.\\n\\nSecurity and Privacy: Protecting data and keeping it private.\\n\\nResource Management: Using computer resources like memory and power wisely.\\n\\nVersioning and Model Management: Keeping track of different versions of the model.\\n\\nRegulatory Compliance: Making sure the model follows the laws, rules, and regulations.\\n\\nUser Acceptance and Trust: Getting people to trust and accept the model.\\n\\nExplainability and Transparency: Being able to explain how the model works.\\n\\nCost Management: Managing how much it costs to use the model.\\n\\nAs per research, only 13% of ML models ever make it to production. This is a huge gap, considering the possibilities that AI model deployment can bring to the organization.\\n\\n',\n",
       " 7: '\\n\\nData and Model Management\\n\\nData Pipelines: Building and maintaining data pipelines for continuous data flow.\\n\\nModel Versioning: Tracking and managing different versions of models.\\n\\n',\n",
       " 8: '\\n\\nA/B Testing\\n\\nObjective Comparison: A/B testing allows for an objective comparison of two model versions to determine which performs better based on specific metrics.\\n\\nReal-World Application: It is widely used to optimize user experiences, such as testing different recommendation systems or ad strategies to enhance engagement or conversion rates.\\n\\nStatistical Significance: The technique ensures that performance differences are statistically significant and not due to random chance by using control and treatment groups along with statistical tests.\\n\\n',\n",
       " 9: '\\n\\nSecurity, Compliance and Bias\\n\\nSecurity: Ensuring the security of machine learning models involves protecting sensitive data from unauthorized access and breaches through robust encryption, secure APIs, and access controls\\n\\nCompliance: Adhering to industry regulations and standards, such as GDPR or HIPAA, is critical to ensure the legal and ethical use of data in machine learning deployments. This involves data anonymization, user consent, and regular compliance audits.\\n\\nBias Detection: Identifying and mitigating bias in ML models is crucial to prevent unfair and discriminatory outcomes. This involves using diverse training datasets, applying fairness-aware algorithms, and conducting bias impact assessments\\n\\nContinuous Monitoring: Regular monitoring and updating of deployed models are essential to maintain security, compliance, and fairness. This involves real-time performance tracking, automated alerts for anomalies, and periodic model retraining.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\n",
    "for page, content in ppt_data.items():\n",
    "    context += f\"### Slide {page}:\\n\\n{content.strip()}\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM Code\n",
    "from scripts import llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"\"\"\n",
    "For each PowerPoint slide provided above, write a 2-minute script that effectively conveys the key points.\n",
    "Ensure a smooth flow between slides, maintaining a clear and engaging narrative.\n",
    "\"\"\"\n",
    "\n",
    "response = llm.ask_llm(context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)\n",
    "with open(\"data/ppt_script.md\", \"w\") as f:\n",
    "    f.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 2: Excel Data Analysis with LLM \n",
    "**Note:** Currently LLMs are not good in Math and Data Analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import  UnstructuredExcelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Name Last Name City Gender Brandon James Miami M Sean Hawkins Denver M Judy Day Los Angeles F Ashley Ruiz San Francisco F Stephanie Gomez Portland F'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredExcelLoader(\"data/sample.xlsx\",  mode=\"elements\")\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs)\n",
    "\n",
    "doc = docs[0]\n",
    "doc.metadata\n",
    "\n",
    "doc.page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = doc.metadata['text_as_html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table><tr><td>First Name</td><td>Last Name</td><td>City</td><td>Gender</td></tr><tr><td>Brandon</td><td>James</td><td>Miami</td><td>M</td></tr><tr><td>Sean</td><td>Hawkins</td><td>Denver</td><td>M</td></tr><tr><td>Judy</td><td>Day</td><td>Los Angeles</td><td>F</td></tr><tr><td>Ashley</td><td>Ruiz</td><td>San Francisco</td><td>F</td></tr><tr><td>Stephanie</td><td>Gomez</td><td>Portland</td><td>F</td></tr></table>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| First Name | Last Name | City         | Gender |\n",
      "|------------|-----------|--------------|--------|\n",
      "| Brandon    | James     | Miami        | M      |\n",
      "| Sean       | Hawkins   | Denver       | M      |\n",
      "| Judy       | Day       | Los Angeles  | F      |\n",
      "| Ashley     | Ruiz      | San Francisco| F      |\n",
      "| Stephanie  | Gomez     | Portland     | F      |\n"
     ]
    }
   ],
   "source": [
    "question = \"Return this data in Markdown format.\"\n",
    "response = llm.ask_llm(context, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| First Name | Last Name | City         | Gender |\n",
      "|------------|-----------|--------------|--------|\n",
      "| Judy       | Day       | Los Angeles  | F      |\n",
      "| Ashley     | Ruiz      | San Francisco| F      |\n",
      "| Stephanie  | Gomez     | Portland     | F      |\n"
     ]
    }
   ],
   "source": [
    "question = \"Return all entris in the table where Gender is 'F'. Format the response in Markdown. Do not write preambles and explanation.\"\n",
    "response = llm.ask_llm(context, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| First Name | Last Name | City       | Gender |\n",
      "|------------|-----------|------------|--------|\n",
      "| Brandon    | James     | Miami      | M      |\n",
      "| Sean       | Hawkins   | Denver     | M      |\n"
     ]
    }
   ],
   "source": [
    "question = \"Return all entris in the table where Gender is 'male'. Format the response in Markdown. Do not write preambles and explanation.\"\n",
    "response = llm.ask_llm(context, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 3: Personalized Job Application Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import  Docx2txtLoader\n",
    "\n",
    "loader = Docx2txtLoader(\"data/job_description.docx\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Subject:** Application for Data Scientist Position  \n",
      "\n",
      "Dear SpiceJet Team,  \n",
      "\n",
      "My name is Aaditya, a recent graduate from IIT with a focus on Natural Language Processing and Machine Learning. I am applying for the Data Scientist role at SpiceJet, as outlined in your job description.  \n",
      "\n",
      "With a strong foundation in data science, machine learning, and NLP, I am eager to contribute to your mission of leveraging data to drive revenue growth, reduce costs, and enhance customer experiences. My proficiency in Python, R, SQL, and tools like Tableau aligns with your requirements, and I have experience in predictive modeling and statistical analysis.  \n",
      "\n",
      "I am particularly drawn to SpiceJetâ€™s emphasis on collaboration with product teams and deploying models to automate processes. My academic projects and coursework in ML and data-driven systems have equipped me to translate complex problems into actionable insights, ensuring high-quality, scalable solutions.  \n",
      "\n",
      "I am excited about the opportunity to join your dynamic team and contribute to innovative data science initiatives. Thank you for considering my application.  \n",
      "\n",
      "Best regards,  \n",
      "Aaditya\n"
     ]
    }
   ],
   "source": [
    "question =\"\"\"\n",
    "My name is Aaditya, and I am a recent graduate from IIT with a focus on Natural Language Processing and Machine Learning.\n",
    "I am applying for a Data Scientist position at SpiceJet.\n",
    "Please write a concise job application email for me in short, removing any placeholders, including references to job boards or sources.\n",
    "\"\"\"\n",
    "response = llm.ask_llm(context, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-and-ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
