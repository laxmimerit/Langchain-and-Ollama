{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ¤– LLM and AI Agent Development Courses\n\n| Course | Description | Enroll |\n|--------|-------------|---------|\n| **ðŸŽ¯ Master OpenAI Agent Builder** | Build and deploy AI agents visually using OpenAI Agent Builder, ChatKit, RAG, Chatbot, AI Assistant with MCP, AWS, RDS MySQL | [Enroll Now](https://www.udemy.com/course/master-openai-agent-builder-low-code-ai-projects-workflow/?referralCode=B0B67D18B1013E488FB7) |\n| **ðŸ”¥ MCP Mastery** | Build MCP servers & clients with Python, Streamlit, ChromaDB, LangChain, LangGraph agents, and Ollama integrations | [Enroll Now](https://www.udemy.com/course/mcp-mastery-build-ai-apps-with-claude-langchain-and-ollama/?referralCode=31C17C306A59601B8689) |\n| **ðŸ“Š Agentic RAG with LangChain** | Step-by-Step Guide to RAG with LangChain v1, LangGraph, and Ollama (Qwen3, Gemma3, DeepSeek-R1, LLAMA, FAISS) | [Enroll Now](https://www.udemy.com/course/agentic-rag-with-langchain-and-langgraph/?referralCode=C0BCC208F53AF2C98AC5) |\n| **ðŸ”§ Master LangGraph and LangChain** | Agentic RAG and Chatbot, AI Agent with LangChain v1, Qwen3, Gemma3, DeepSeek-R1, LLAMA 3.2, FAISS Vector Database | [Enroll Now](https://www.udemy.com/course/langgraph-with-ollama/?referralCode=B646DCB44A189BEBC20C) |\n| **âš¡ Master Langchain and Ollama** | Master Langchain v1, Local LLM Projects with Ollama, Qwen3, Gemma3, DeepSeek-R1, LLAMA 3.2, Complete Integration Guide | [Enroll Now](https://www.udemy.com/course/ollama-and-langchain/?referralCode=7F4C0C7B8CF223BA9327) |\n| **ðŸ”¬ Fine Tuning LLM** | Learn transformer architecture fundamentals and fine-tune LLMs with custom datasets | [Enroll Now](https://www.udemy.com/course/fine-tuning-llm-with-hugging-face-transformers/?referralCode=6DEB3BE17C2644422D8E) |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Expression Language Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  LangChain Expression Language is that any two runnables can be \"chained\" together into sequences. \n",
    "- The output of the previous runnable's .invoke() call is passed as input to the next runnable.\n",
    "- This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Type of LCEL Chains\n",
    "    - SequentialChain\n",
    "    - Parallel Chain\n",
    "    - Router Chain\n",
    "    - Chain Runnables\n",
    "    - Custom Chain (Runnable Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='qwen3', base_url='http://localhost:11434')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import (\n",
    "                                        SystemMessagePromptTemplate,\n",
    "                                        HumanMessagePromptTemplate,\n",
    "                                        ChatPromptTemplate\n",
    "                                        )\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'qwen3'\n",
    "\n",
    "llm = ChatOllama(base_url=base_url, model=model)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Sun is the center, holding the solar system together with gravity.  \n",
      "2. Eight planets orbit the Sun: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.  \n",
      "3. The asteroid belt lies between Mars and Jupiter, with rocky objects.  \n",
      "4. Gas giants (Jupiter, Saturn, Uranus, Neptune) are large and mostly made of gas.  \n",
      "5. Distant regions like the Kuiper Belt and Oort Cloud contain icy bodies and comets.\n"
     ]
    }
   ],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "question = template.invoke({'school': 'primary', 'topics': 'solar system', 'points': 5})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "chain = template | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Sun is the center, holding the solar system together with gravity.  \n",
      "2. Eight planets orbit the Sun: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.  \n",
      "3. The asteroid belt lies between Mars and Jupiter, with rocky objects.  \n",
      "4. Moons orbit planets, like Earthâ€™s Moon and Jupiterâ€™s many moons.  \n",
      "5. The Kuiper Belt and Oort Cloud are regions of icy bodies beyond Neptune.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 5})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Sun is the central star, providing gravity and energy.  \n",
      "2. Eight planets orbit it: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune.  \n",
      "3. Smaller bodies include asteroids, comets, and dwarf planets like Pluto.  \n",
      "4. The solar system has distinct regions: inner rocky planets, outer gas giants, asteroid belt, and Kuiper Belt.  \n",
      "5. It formed from a collapsing cloud of gas and dust around 4.6 billion years ago.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'school': 'phd', 'topics': 'solar system', 'points': 5})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. The Sun is the central star, providing gravity and energy.  \\n2. Eight planets orbit it: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune.  \\n3. Smaller bodies include asteroids, comets, and dwarf planets like Pluto.  \\n4. The solar system has distinct regions: inner rocky planets, outer gas giants, asteroid belt, and Kuiper Belt.  \\n5. It formed from a collapsing cloud of gas and dust around 4.6 billion years ago.', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-10-22T06:48:23.0623152Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2236060800, 'load_duration': 69630100, 'prompt_eval_count': 37, 'prompt_eval_duration': 15459300, 'eval_count': 402, 'eval_duration': 2052522300, 'model_name': 'qwen3', 'model_provider': 'ollama'}, id='lc_run--d02d48ee-498c-4a32-949a-dceb964ae040-0', usage_metadata={'input_tokens': 37, 'output_tokens': 402, 'total_tokens': 439})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Sun is the center, holding the solar system together with gravity.  \n",
      "2. Eight planets orbit the Sun: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.  \n",
      "3. The asteroid belt lies between Mars and Jupiter, with rocky objects.  \n",
      "4. Moons orbit planets, like Earthâ€™s Moon and Jupiterâ€™s many moons.  \n",
      "5. Distant regions include the Kuiper Belt and Oort Cloud, home to icy bodies.\n"
     ]
    }
   ],
   "source": [
    "chain = template | llm | StrOutputParser()\n",
    "response = chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 5})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. The Sun is the center, holding the solar system together with gravity.  \\n2. Eight planets orbit the Sun: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.  \\n3. The asteroid belt lies between Mars and Jupiter, with rocky objects.  \\n4. Moons orbit planets, like Earthâ€™s Moon and Jupiterâ€™s many moons.  \\n5. Distant regions include the Kuiper Belt and Oort Cloud, home to icy bodies.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Runnables (Chain Multiple Runnables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can even combine this chain with more runnables to create another chain.\n",
    "- Let's see how easy our generated output is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['points', 'school', 'topics'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['school'], input_types={}, partial_variables={}, template='You are {school} teacher. You answer in short sentences.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['points', 'topics'], input_types={}, partial_variables={}, template='tell me about the {topics} in {points} points'), additional_kwargs={})])\n",
       "| ChatOllama(model='qwen3', base_url='http://localhost:11434')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is easy to understand as it presents basic, clear information about the solar system using simple language and familiar concepts.\n"
     ]
    }
   ],
   "source": [
    "analysis_prompt = ChatPromptTemplate.from_template('''analyze the following text: {response}\n",
    "                                                   You need tell me that how difficult it is to understand.\n",
    "                                                   Answer in one sentence only.\n",
    "                                                   ''')\n",
    "\n",
    "fact_check_chain = analysis_prompt | llm | StrOutputParser()\n",
    "output = fact_check_chain.invoke({'response': response})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is relatively simple and accessible, requiring basic knowledge of astronomy concepts like planetary orbits, dwarf planets, and the solar system's structure.\n"
     ]
    }
   ],
   "source": [
    "composed_chain = {\"response\": chain} | analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "output = composed_chain.invoke({'school': 'phd', 'topics': 'solar system', 'points': 5})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel LCEL Chain\n",
    "- Parallel chains are used to run multiple runnables in parallel.\n",
    "- The final return value is a dict with the results of each value under its appropriate key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system has the Sun at its center, with eight planets orbiting around it.  \n",
      "2. It includes moons, asteroids, comets, and other celestial objects in space.\n"
     ]
    }
   ],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "fact_chain = template | llm | StrOutputParser()\n",
    "\n",
    "output = fact_chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 2})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun shines bright, a golden sphere,  \n",
      "Planets dance in silent cheer.\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessagePromptTemplate.from_template('write a poem on {topics} in {sentences} lines')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "poem_chain = template | llm | StrOutputParser()\n",
    "\n",
    "output = poem_chain.invoke({'school': 'primary', 'topics': 'solar system', 'sentences': 2})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(fact = fact_chain, poem = poem_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system has the Sun at its center, with eight planets orbiting around it.  \n",
      "2. It includes moons, asteroids, comets, and dwarf planets like Pluto, all held by gravity.\n",
      "\n",
      "\n",
      "\n",
      "The sun, a fiery heart, spins bright and bold,  \n",
      "planets dance in orbits, each a world of gold.\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 2, 'sentences': 2})\n",
    "print(output['fact'])\n",
    "print('\\n\\n')\n",
    "print(output['poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Router\n",
    "- The router chain is used to route the output of a previous runnable to the next runnable based on the output of the previous runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Given the user review below, classify it as either being about `Positive` or `Negative`.\n",
    "            Do not respond with more than one word.\n",
    "\n",
    "            Review: {review}\n",
    "            Classification:\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "review = \"Thank you so much for providing such a great plateform for learning. I am really happy with the service.\"\n",
    "# review = \"I am not happy with the service. It is not good.\"\n",
    "chain.invoke({'review': review})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_prompt = \"\"\"\n",
    "                You are expert in writing reply for positive reviews.\n",
    "                You need to encourage the user to share their experience on social media.\n",
    "                Review: {review}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "positive_template = ChatPromptTemplate.from_template(positive_prompt)\n",
    "positive_chain = positive_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"\"\"\n",
    "                You are expert in writing reply for negative reviews.\n",
    "                You need first to apologize for the inconvenience caused to the user.\n",
    "                You need to encourage the user to share their concern on following Email:'udemy@kgptalkie.com'.\n",
    "                Review: {review}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "\n",
    "negative_template = ChatPromptTemplate.from_template(negative_prompt)\n",
    "negative_chain = negative_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rout(info):\n",
    "    if 'positive' in info['sentiment'].lower():\n",
    "        return positive_chain\n",
    "    else:\n",
    "        return negative_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rout({'sentiment': 'negetive'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\"sentiment\": chain, 'review': lambda x: x['review']} | RunnableLambda(rout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  sentiment: ChatPromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='Given the user review below, classify it as either being about `Positive` or `Negative`.\\n            Do not respond with more than one word.\\n\\n            Review: {review}\\n            Classification:'), additional_kwargs={})])\n",
       "             | ChatOllama(model='qwen3', base_url='http://localhost:11434')\n",
       "             | StrOutputParser(),\n",
       "  review: RunnableLambda(lambda x: x['review'])\n",
       "}\n",
       "| RunnableLambda(rout)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We sincerely apologize for the inconvenience caused and understand your dissatisfaction. We value your feedback and kindly ask you to share your concern via email at **udemy@kgptalkie.com** so that we can address your issue promptly. We are committed to resolving this matter to your satisfaction and appreciate your patience as we work to improve our services. Thank you for bringing this to our attention.\n"
     ]
    }
   ],
   "source": [
    "# review = \"Thank you so much for providing such a great plateform for learning. I am really happy with the service.\"\n",
    "review = \"I am not happy with the service. It is not good.\"\n",
    "\n",
    "output = full_chain.invoke({'review': review})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Custom Chain Runnables with RunnablePassthrough and RunnableLambda\n",
    "- This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_counts(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_counts(text):\n",
    "    return len(text.split())\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain these inputs in 5 sentences: {input1} and {input2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input1', 'input2'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input1', 'input2'], input_types={}, partial_variables={}, template='Explain these inputs in 5 sentences: {input1} and {input2}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Earth is a planet, meaning it is a celestial body that orbits the Sun, has a solid surface, and meets specific criteria like clearing its orbit of other debris.  \n",
      "2. The Sun is a star, a massive, luminous sphere of plasma held together by gravity, where nuclear fusion powers its light and heat.  \n",
      "3. Planets, like Earth, do not produce their own light but reflect sunlight, while stars, like the Sun, generate energy through nuclear reactions.  \n",
      "4. Earthâ€™s classification as a planet distinguishes it from stars, which are much larger and emit light due to internal heat and fusion.  \n",
      "5. The Sunâ€™s role as a star is central to sustaining life on Earth, as its energy drives weather, climate, and the planetâ€™s orbital motion.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "output = chain.invoke({'input1': 'Earth is planet', 'input2': 'Sun is star'})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'char_counts': 719, 'word_counts': 122, 'output': '1. Earth is a planet, meaning it is a celestial body that orbits the Sun and is characterized by its solid surface, atmosphere, and ability to support life.  \\n2. The Sun is a star, a massive, luminous sphere of plasma held together by gravity, which generates energy through nuclear fusion in its core.  \\n3. Planets like Earth are much smaller and cooler than stars, and they do not produce their own light but reflect sunlight.  \\n4. The Sunâ€™s gravitational pull keeps Earth and other planets in orbit, forming the solar system.  \\n5. While Earth is a planet, the Sunâ€™s status as a star highlights the fundamental difference between these two types of celestial objects in terms of size, composition, and energy sources.'}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser() | {'char_counts': RunnableLambda(char_counts), \n",
    "                                            'word_counts': RunnableLambda(word_counts), \n",
    "                                            'output': RunnablePassthrough()}\n",
    "\n",
    "output = chain.invoke({'input1': 'Earth is planet', 'input2': 'Sun is star'})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Chain using `@chain` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system has the Sun at its center.  \n",
      "2. Eight planets orbit the Sun in elliptical paths.\n",
      "\n",
      "\n",
      "\n",
      "The sun reigns bright, a golden core,  \n",
      "Planets dance in orbits, vast and wide.\n"
     ]
    }
   ],
   "source": [
    "@chain\n",
    "def custom_chain(params):\n",
    "    return {\n",
    "        'fact': fact_chain.invoke(params),\n",
    "        'poem': poem_chain.invoke(params),\n",
    "    }\n",
    "\n",
    "\n",
    "params = {'school': 'primary', 'topics': 'solar system', 'points': 2, 'sentences': 2}\n",
    "output = custom_chain.invoke(params)\n",
    "print(output['fact'])\n",
    "print('\\n\\n')\n",
    "print(output['poem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-and-ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}